<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
        <title>llama2.rs.wasm</title>
        <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/water.css@2/out/dark.css"
        />
        <link
        rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css"
        integrity="sha512-z3gLpd7yknf1YoNbCzqRKc4qyor8gaKU1qmn+CShxbuBusANI9QpRohGBreCFkKxLhei6S9CQXFEbbKuqLg0DA=="
        crossorigin="anonymous"
        referrerpolicy="no-referrer"
        />

        <!-- preload loads 438m+167m+60m+size_of(port2_bg.wasm) ... after few refresh browser memory goes up fast -->
        <link rel="preload" href="/pkg/llama2_rs_wasm_bg.wasm" as="fetch" type="application/wasm"/>
        <script>
            const stories = { 
                stories15M : "https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin",
                stories42M : "https://huggingface.co/karpathy/tinyllamas/resolve/main/stories42M.bin",
                stories110M : "https://huggingface.co/karpathy/tinyllamas/resolve/main/stories110M.bin",
            };

            const tokenizers = { 
                //port1 : "https://cdn.jsdelivr.net/gh/gaxler/llama2.rs/tokenizer.bin",
                port1: "port1_tokenizer.bin",
                //port2 : "https://cdn.jsdelivr.net/gh/leo-du/llama2.rs/tokenizer.bin"
                port2: "port2_tokenizer.bin",
                //port3 : "https://cdn.jsdelivr.net/gh/danielgrittner/llama2-rs/tokenizer.bin"
                port3: "port3_tokenizer.bin",
                //port4 : "https://cdn.jsdelivr.net/gh/lintian06/llama2.rs/tokenizer.bin"
                port4: "port4_tokenizer.bin",
                //port5 : "https://cdn.jsdelivr.net/gh/rahoua/pecca-rs/tokenizer.bin"
                port5: "port5_tokenizer.bin",
                //port6 : "https://cdn.jsdelivr.net/gh/flaneur2020/llama2.rs/tokenizer.bin"
                port6: "port4_tokenizer.bin",
            };

            function log(msg,...args){
                if (document.getElementById("debug_info").checked) {
                    document.getElementById("debug_content1").value+=msg;
                }
            }

            function enabledebug(){
                //console.log(this);
                document.getElementById("debug_content1").value="";
                if (!document.getElementById("debug_info").checked) {
                    document.getElementById("debug_content").style="display:none;";
                }else {
                    document.getElementById("debug_content").style="";
                }
            }

            function onchange(){
                //console.log(this);
                document.getElementById("debug_content1").value="";
                document.getElementById("prompt").value="";
            }

            function onmodelchange(){
                onchange();
            }
            function onimplchange(){
                onchange();
            }
        </script>     
    </head>
    <body>
        <div class="container">
            <h1>llama2.rs.wasm</h1>
    
            <h3>Links</h3>
            <p>
            <a
                target="_blank"
                rel="noopener noreferrer"
                href="https://github.com/mtb0x1/llama2.rs.wasm"
                style="all: unset; cursor: pointer; font-size: 0.9em"
            >
            <i class="fa-brands fa-github fa-xl"></i>
            llama2.rs.wasm Source Code
            </a>
            </p>

            <label for="WhichImpl">Which port</label>
            <select id="WhichImpl" onchange="onimplchange();">
                <option value="port1">port1 - @gaxler *prompt not supported</option>
                <option selected="selected" value="port2">port2 - @leo-du</option>
                <option value="port3">port3 - @danielgrittner</option>
                <option value="port4">port4 - @lintian06</option>
                <option value="port5">port5 - @rahoua</option>
                <option value="port6">port6 - @flaneur2020</option>
            </select>

            <label for="model">Model</label>
            <select id="model" onchange="onmodelchange();">
                <option value="stories15M">stories15m</option>
                <option selected="selected" value="stories42M">stories42m</option>
                <option value="stories110M">stories110m</option>
            </select>
            <label for="temperature">Temperature</label>
            <input id="temperature" type="number" value="0.9" step=".01" min="0" max="1"  /> 
            <label for="steps">Steps</label>
            <input id="steps" type="number" value="20" />
            <label for="debug_info">Debug info:</label>
            <input id="debug_info" type="checkbox" onchange="enabledebug()"/></br>
            <label for="prompt">Prompt:</label>
            <textarea id="prompt" name="prompt" rows="10" placeholder="write something here ..."></textarea>
            <div id="debug_content" style="display:none">
            <label for="debug_content1">Debug content:</label>
            <textarea id="debug_content1" name="prompt" rows="10" disabled></textarea>
            </div>
            <button id="run" class="flat">Run</button>
            <div id="meta">
                <label for="token_per_second">Token per second:</label>
                <input id="token_per_second" type="number" value="0" disabled/>
                <p>*for acurate measure, close other tabs and restart browser.
                    Performance is reduced by 10x after a couple of runs.</p>
            </div>
        </div>
    <script type="module">
        import init, { main_wasm, initThreadPool  } from './pkg/llama2_rs_wasm.js';
        await init();
        if (navigator.hardwareConcurrency >0){
            const cpus_in_use = Math.ceil(navigator.hardwareConcurrency*0.75);
            //requires https://stackoverflow.com/questions/72881660/web-worker-blocked-by-self-crossoriginisolated-on-cypress
            //Cross-Origin-Opener-Policy="same-origin"
            //Cross-Origin-Embedder-Policy="require-corp"
            //Fixme(https://github.com/mtb0x1/llama2.rs.wasm/issues/1)
            //await initThreadPool(cpus_in_use);
        }

        async function run() {
            let model = stories[document.getElementById("model").value];
            const model_buffer =await fetch(model)
            .then(response => response.arrayBuffer())
            .then(buffer => {
                // Convert the ArrayBuffer to a Uint8Array
                return new Uint8Array(buffer);
            });
            log("JS: model_buffer fetched\n");  
            let tokenizer = tokenizers[document.getElementById("WhichImpl").value];
            const tokenizer_buffer =await fetch(tokenizer)
            .then(response => response.arrayBuffer())
            .then(buffer => {
                // Convert the ArrayBuffer to a Uint8Array
                return new Uint8Array(buffer);
            });
            log("JS: tokenizer_buffer fetched\n");
            
            let temperature  = document.getElementById("temperature").value;
            let steps = document.getElementById("steps").value;
            let prompt = document.getElementById("prompt").value?? ""; 
            let WhichImpl = document.getElementById("WhichImpl").value;

            let result = main_wasm(model_buffer,tokenizer_buffer,temperature,steps,prompt,WhichImpl);
            document.getElementById("prompt").value = result.result?? "";
            document.getElementById("token_per_second").value = result.token_per_second?? 0;
            log(result.logs?? "");
        }
        document.getElementById('run').addEventListener('click', run);
    </script>

    </body>
</html>
